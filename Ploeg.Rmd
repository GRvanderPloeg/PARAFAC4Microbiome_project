---
title: "Ploeg"
author: "G.R. van der Ploeg"
date: "2024-03-12"
output: html_document
---

```{r setup}
library(parafac4microbiome)
library(tidyverse)
library(multiway)
library(ggpubr)
set.seed(123)
```

```{r prep plot data}
# Plot settings
colourCols = c("RFgroup", "Phylum", "")
legendTitles = c("RF group", "Phylum", "")
xLabels = c("Subject index", "Feature index", "Time index")
legendColNums = c(3,5,0)
arrangeModes = c(TRUE, TRUE, FALSE)
continuousModes = c(FALSE,FALSE,TRUE)
```

```{r check sparsity}
sparsity = calculateSparsity(vanderPloeg2024, considerGroups = TRUE, groupVariable = "RFgroup") * 100

a=sparsity[1,] %>% as_tibble() %>% ggplot(aes(x=value)) + geom_histogram(col="black", bins=25) + geom_vline(xintercept=50, col="red", linewidth=1) + xlab("Sparsity (%)") + ylab("Count") + ggtitle("Low responders")
b=sparsity[2,] %>% as_tibble() %>% ggplot(aes(x=value)) + geom_histogram(col="black", bins=25) + geom_vline(xintercept=50, col="red", linewidth=1) +xlab("Sparsity (%)") + ylab("Count") + ggtitle("Mid responders")
c=sparsity[3,] %>% as_tibble() %>% ggplot(aes(x=value)) + geom_histogram(col="black", bins=25) + geom_vline(xintercept=50, col="red", linewidth=1) +xlab("Sparsity (%)") + ylab("Count") + ggtitle("High responders")
ggarrange(a,b,c, nrow=1)
```

```{r process the data}
processedPloeg = processDataCube(vanderPloeg2024, sparsityThreshold=0.5, considerGroups=TRUE, groupVariable="RFgroup", centerMode=1, scaleMode=2)
```

```{r investigate num comp}
# numRepetitions = 50
# assessment = assessNumComponents(processedPloeg$data, minNumComponents=1, maxNumComponents=5, numRepetitions=numRepetitions, ctol=1e-6, maxit=500, numCores=12)
```

```{r check the plots}
# assessment$plots$overview
# assessment$plots$TCC[[3]]
```

```{r model stability}
# numFolds = 50
# 
# stability2 = modelStabilityCheck(processedPloeg, numComponents=2, numFolds=numFolds, colourCols=colourCols, legendTitles=legendTitles, xLabels=xLabels, legendColNums=legendColNums, arrangeModes=arrangeModes, continuousModes=continuousModes, numCores=12)
# stability3 = modelStabilityCheck(processedPloeg, numComponents=3, numFolds=numFolds, colourCols=colourCols, legendTitles=legendTitles, xLabels=xLabels, legendColNums=legendColNums, arrangeModes=arrangeModes, continuousModes=continuousModes, numCores=12)
# 
# stability2$plot
# stability3$plot
```

```{r select the model}
# numComponents = 2
# modelChoice = which(assessment$metrics$varExp[,numComponents] == max(assessment$metrics$varExp[,numComponents]))
# finalModel = assessment$models[[numComponents]][[modelChoice]]
# 
# # Correct time loadings
# finalModel = resign(finalModel, mode="B", absorb="A")
# 
# # Save the model
# saveRDS(finalModel, "./Ploeg2024.RDS")

# Load the model
finalModel = readRDS("./Ploeg2024.RDS")
```

```{r plot the model}
# Plot 4B
varExp = calculateVarExp(finalModel, processedPloeg$data)

plotlist = plotPARAFACmodel(finalModel, processedPloeg, colourCols, legendTitles, xLabels, legendColNums, arrangeModes,
  continuousModes = c(FALSE,FALSE,TRUE),
  overallTitle = "")
plotPARAFACmodel(finalModel, processedPloeg, colourCols, legendTitles, xLabels, legendColNums, arrangeModes,
  continuousModes = c(FALSE,FALSE,TRUE),
  overallTitle = " ")

newPlotlist = list()
for(i in 1:6){
  
  if(i==3 | i==6){
    newPlotlist[[i]] = plotlist[[i]] + theme(text=element_text(size=14)) + scale_x_continuous(breaks=1:7,labels=c("-14","0","2","5","9","14","21"))
  }else{
    newPlotlist[[i]] = plotlist[[i]] + theme(text=element_text(size=14))
  }
  
}

ggarrange(plotlist=newPlotlist, ncol=3, nrow=2)
```
```{r Import red fluorescence data}
rf_data = read.csv("./vanderPloeg2024/Homogenized data analysis/0. Raw data input/RFdata.csv")
colnames(rf_data) = c("subject", "id", "fotonr", "day", "group", "RFgroup", "MQH", "SPS(tm)", "Area_delta_R30", "Area_delta_Rmax", "Area_delta_R30_x_Rmax", "gingiva_mean_R_over_G", "gingiva_mean_R_over_G_upper_jaw", "gingiva_mean_R_over_G_lower_jaw")
rf_data = rf_data %>% as_tibble()

rf_data[rf_data$subject == "VSTPHZ", 1] = "VSTPH2"
rf_data[rf_data$subject == "D2VZH0", 1] = "DZVZH0"
rf_data[rf_data$subject == "DLODNN", 1] = "DLODDN"
rf_data[rf_data$subject == "O3VQFX", 1] = "O3VQFQ"
rf_data[rf_data$subject == "F80LGT", 1] = "F80LGF"
rf_data[rf_data$subject == "26QQR0", 1] = "26QQrO"

rf_data2 = read.csv("./vanderPloeg2024/Homogenized data analysis/0. Raw data input/red_fluorescence_data.csv") %>% as_tibble()
rf_data2 = rf_data2[,c(2,4,181:192)]
rf_data = rf_data %>% left_join(rf_data2)

rf = rf_data %>% select(subject, RFgroup) %>% unique()
```

```{r Import subject metadata}
age_gender = read.csv("./vanderPloeg2024/Homogenized data analysis/0. Raw data input/20160210 demografische gegevens TIFN2.csv", sep=";")
age_gender = age_gender[2:nrow(age_gender),2:ncol(age_gender)]
age_gender = age_gender %>% as_tibble() %>% filter(onderzoeksgroep == 0) %>% select(naam, leeftijd, geslacht)
colnames(age_gender) = c("subject", "age", "gender")

# Correction for incorrect subject ids
age_gender[age_gender$subject == "VSTPHZ", 1] = "VSTPH2"
age_gender[age_gender$subject == "D2VZH0", 1] = "DZVZH0"
age_gender[age_gender$subject == "DLODNN", 1] = "DLODDN"
age_gender[age_gender$subject == "O3VQFX", 1] = "O3VQFQ"
age_gender[age_gender$subject == "F80LGT", 1] = "F80LGF"
age_gender[age_gender$subject == "26QQR0", 1] = "26QQrO"

age_gender = age_gender %>% arrange(subject)
```

```{r test metadata}
normalSubjectLoadings = cbind(finalModel$A, processedPloeg$mode1) %>% as_tibble()
transformedSubjectLoadings = correctPARAFACloadings(processedPloeg, finalModel, 2, moreOutput=TRUE)$Ftilde %>% as_tibble() %>% mutate(subject = rep(processedPloeg$mode1$subject, each=7), day = rep(c(-14,0,2,5,9,14,21),41))

# Test against RFgroup (for completeness)
normalSubjectLoadings %>% select(1,2,RFgroup) %>% pivot_longer(-RFgroup) %>% ggplot(aes(x=as.factor(RFgroup),y=value)) + facet_wrap(vars(name)) + geom_boxplot() + stat_compare_means()

# Plaque%
temp=transformedSubjectLoadings %>% left_join(rf_data %>% select(subject, day, plaquepercent))
cor.test(temp$V1, temp$plaquepercent, method="pearson")
cor.test(temp$V2, temp$plaquepercent, method="pearson")

# Bleeding%
temp=transformedSubjectLoadings %>% left_join(rf_data %>% select(subject, day, bomppercent))
cor.test(temp$V1, temp$bomppercent, method="pearson")
cor.test(temp$V2, temp$bomppercent, method="pearson")

# RF%
temp=transformedSubjectLoadings %>% left_join(rf_data %>% select(subject, day, Area_delta_R30))
cor.test(temp$V1, temp$Area_delta_R30, method="pearson")
cor.test(temp$V2, temp$Area_delta_R30, method="pearson")

# Gender
partA = normalSubjectLoadings %>% left_join(age_gender) %>% select(1,2,gender) %>% filter(gender==1)
partB = normalSubjectLoadings %>% left_join(age_gender) %>% select(1,2,gender) %>% filter(gender==2)

t.test(partA$`1`, partB$`1`) # Not what I would do nowadays, but this is what is in the TIFN paper
t.test(partA$`2`, partB$`2`)

# Age
temp = normalSubjectLoadings %>% left_join(age_gender) %>% select(1,2,age)
cor.test(temp$`1`, temp$age, method="pearson")
cor.test(temp$`2`, temp$age, method="pearson")

uncorrectedP = c(0.092, 0.0044, 0.007492, 0.0154, 0.2681, 0.184, 1.317e-6, 0.01367, 0.4936, 0.5902, 0.7222, 0.769)
correctedP = p.adjust(uncorrectedP, "BH")

matrix(uncorrectedP, nrow=2)
matrix(correctedP, nrow=2)
```

```{r create relabs plot}
# NOTE: does not filter based on the model yet
# Currently scrapped because it adds no value to the paper

# I = dim(vanderPloeg2024$data)[1]
# J = dim(vanderPloeg2024$data)[2]
# K = dim(vanderPloeg2024$data)[3]
# 
# countMatrix = matrix(vanderPloeg2024$data, nrow=I)
# newColNames = paste0(rep(vanderPloeg2024$mode2$asv,K), "_t", rep(1:K, each=J))
# colnames(countMatrix) = newColNames
# countMatrix = countMatrix %>% as_tibble() %>% mutate(subject=vanderPloeg2024$mode1$subject) %>% pivot_longer(-subject) %>% mutate(timepoint=as.numeric(str_split_fixed(name,"_t",2)[,2]),id=str_split_fixed(name,"_t",2)[,1]) %>% select(-name) %>% pivot_wider(names_from=id,values_from=value) # shenanigans to create an I*K x J matrix
# countMatrix.numeric = countMatrix %>% select(-subject,-timepoint)
# 
# totalSums = rowSums(countMatrix.numeric)
# relAbs = sweep(countMatrix.numeric, 1, totalSums, FUN="/") %>% as_tibble()
# 
# # Plot 4A
# relAbs %>% mutate(subject=countMatrix$subject, timepoint=countMatrix$timepoint) %>% pivot_longer(-c(subject,timepoint)) %>% left_join(vanderPloeg2024$mode1) %>% left_join(vanderPloeg2024$mode2, by=c("name"="asv")) %>% group_by(subject,timepoint,Phylum) %>% summarize(s=sum(value)) %>% left_join(vanderPloeg2024$mode1) %>% ungroup() %>% group_by(RFgroup,Phylum,timepoint) %>% summarize(m=mean(s,na.rm=TRUE)) %>% ungroup() %>% ggplot(aes(x=as.factor(timepoint),y=m,fill=as.factor(Phylum))) + facet_wrap(vars(RFgroup),nrow=3,strip.position="right") + geom_bar(stat="identity") + xlab("Time point [days]") + ylab("Relative abundance") + theme(legend.position="none")
```


```{r filter features for clustering}
conLoad = function(X, model, mode){
  if(mode == 1){
    Z = multiway::krprod(finalModel$C, finalModel$B)
    numComponents = 2
    congruences = matrix(0, nrow=I, ncol=numComponents)
    
    for(f in 1:numComponents){
      vectZ = c(Z[,f])
      b = finalModel$B[,f]
      c = finalModel$C[,f]
      
      for(i in 1:I){
        Xi = processedPloeg$data[i,,]
        vectX = c(Xi)
        congruences[i,f] = multiway::congru(vectZ, vectX)
      }
    }
  }
  else if(mode == 2){
    Z = multiway::krprod(finalModel$C, finalModel$A)
    numComponents = 2
    congruences = matrix(0, nrow=J, ncol=numComponents)
    
    for(f in 1:numComponents){
      vectZ = c(Z[,f])
      for(j in 1:J){
        vectX = c(processedPloeg$data[,j,])
        congruences[j,f] = multiway::congru(vectZ, vectX)
      }
    }
  }
  else if(mode == 3){
    Z = multiway::krprod(finalModel$B, finalModel$A)
    numComponents = 2
    congruences = matrix(0, nrow=K, ncol=numComponents)
    
    for(f in 1:numComponents){
      vectZ = c(Z[,f])
      for(k in 1:K){
        vectX = c(processedPloeg$data[,,k])
        congruences[k,f] = multiway::congru(vectZ, vectX)
      }
    }
  }
  return(congruences)
}

# Lazy solution to getting a long matrix
I = dim(processedPloeg$data)[1]
J = dim(processedPloeg$data)[2]
K = dim(processedPloeg$data)[3]
X_long = processedPloeg$data[,,1]
for(k in 2:K){
  X_long = rbind(X_long, processedPloeg$data[,,k])
}
X_wide = matrix(processedPloeg$data, I, J*K)
Xhat = reinflateBlock(finalModel)

# Determine variance explained
featuresVarExp = 1:dim(Xhat)[2]
modelVarExp = varExp

for(i in 1:dim(Xhat)[2]){
  featuresVarExp[i] = multiway::sumsq(Xhat[,i,], na.rm=TRUE) / multiway::sumsq(processedPloeg$data[,i,], na.rm=TRUE)
}

# Determine congruence
featureCongruences = conLoad(processedPloeg$data, finalModel, 2)

# Establish feature mask
varExpThreshold = modelVarExp
congruenceThreshold = 0.4
featureMask = (featuresVarExp >= varExpThreshold) | (rowSums(featureCongruences>=congruenceThreshold) >= 1)
```

```{r cluster features based on modelled data}
library(cluster)
library(factoextra)

Xhat_filtered = Xhat[,featureMask,]

I = dim(Xhat_filtered)[1]
J = dim(Xhat_filtered)[2]
K = dim(Xhat_filtered)[3]

# Lazy solution to getting a long matrix
Xhat_filtered_long = Xhat_filtered[,,1]
for(k in 2:K){
  Xhat_filtered_long = rbind(Xhat_filtered_long, Xhat_filtered[,,k])
}

# Clustering diagnostics
a = fviz_nbclust(t(Xhat_filtered_long), pam, method="wss")
b = fviz_nbclust(t(Xhat_filtered_long), pam, method="silhouette")
c = fviz_nbclust(t(Xhat_filtered_long), pam, method="gap_stat")
ggarrange(a,b,c, nrow=3)

# Cluster
set.seed(1)
numClusters = 3
clusteringResult = pam(t(Xhat_filtered_long), numClusters, nstart=50)
result = processedPloeg$mode2[featureMask,] %>% as_tibble() %>% mutate(cluster=clusteringResult$clustering)

clusteredFeatures = cbind(correctPARAFACloadings(processedPloeg, finalModel, 2), processedPloeg$mode2) %>% as_tibble() %>% left_join(result)
clusteredFeatures %>% ggplot(aes(x=`1`,y=`2`,col=as.factor(cluster))) + geom_point() + xlab("Feature mode, component 1 (transformed)") + ylab("Feature mode, component 2 (transformed)") + scale_color_manual(name = "Cluster", labels=c("1","2","3","Not clustered"), values=c("#F8766D","#00BA38","#619CFF")) + theme(legend.position="bottom",text=element_text(size=14))
```
```{r relabs plots of clusters}
microbiome.raw = read.csv("./vanderPloeg2024/Homogenized data analysis/0. Raw data input/20221005_wp2/count-table.tsv", sep="\t")
taxa = read.csv("./vanderPloeg2024/Homogenized data analysis/0. Raw data input/20221005_wp2/taxonomic-classification.tsv", sep="\t")
selectedIndividuals = microbiome.raw %>% as_tibble() %>% filter(group == "control") %>% select(subject) %>% pull %>% unique

countData = microbiome.raw %>% as_tibble() %>% filter(niche == "upper jaw, lingual", group == "control")
countData.numeric = countData %>% select(-sample,-subject,-visit,-group,-niche)
relAbs = sweep(countData.numeric, 1, rowSums(countData.numeric), FUN="/")
timepoints = c(-14,0,2,5,9,14,21)

clusterStr = c("Cluster 1", "Cluster 2", "Cluster 3")
relAbs %>% as_tibble() %>% mutate(subject=countData$subject,timepoint=timepoints[countData$visit]) %>% pivot_longer(-c(subject,timepoint)) %>% left_join(rf) %>% left_join(clusteredFeatures, by=c("name"="asv")) %>% filter(cluster != "NA") %>% select(subject,timepoint,cluster,name,RFgroup,value) %>% group_by(subject,timepoint,cluster) %>% summarize(s=sum(value)) %>% left_join(rf) %>% ungroup() %>% filter(RFgroup != 1) %>% group_by(RFgroup,timepoint,cluster) %>% summarize(m=mean(s,na.rm=TRUE),v=plotrix::std.error(s,na.rm=TRUE)) %>% ungroup() %>% mutate(cluster_str = clusterStr[cluster]) %>% ggplot(aes(x=as.factor(timepoint),y=m,col=as.factor(RFgroup),group=as.factor(RFgroup))) + facet_grid(cols=vars(cluster_str)) + geom_line() + geom_errorbar(aes(ymax=m+v,ymin=m-v,width=.2)) + geom_point() + theme(legend.position="bottom",text=element_text(size=14)) + xlab("Time point [days]") + ylab("Sum of ASVs per group (mean +/- SEM)") + scale_color_manual(name="Response group",labels=c("Low responders", "High responders"),values=hue_pal()(2))
```

```{r permutation testing of the relabs}
featureClustering = clusteredFeatures %>% mutate(group = cluster) %>% select(-cluster)

ASVgroupPermutationTest = function(nicheName, featureClustering, visitNumber, groupNumber, numPermutations){
  microbiome.numeric = microbiome.raw %>% as_tibble() %>% filter(group=="control", niche==nicheName, visit==visitNumber)
  microbiome.meta = microbiome.numeric %>% select(subject)
  microbiome.numeric = microbiome.numeric %>% select(-sample,-subject,-visit,-group,-niche)
  totalSums = rowSums(microbiome.numeric)
  relativeAbundances = sweep(microbiome.numeric, 1, totalSums, FUN="/") %>% as_tibble() %>% mutate(subject = microbiome.meta$subject)

  df = relativeAbundances %>%
    select(all_of(c(featureClustering$asv, "subject"))) %>%
    pivot_longer(-subject) %>%
    left_join(featureClustering, by=c("name"="asv")) %>%
    group_by(subject, group) %>%
    summarize(s=sum(value), .groups="drop") %>%
    ungroup() %>%
    left_join(rf, by="subject") %>%
    filter(group == groupNumber)

  dfA = df %>% filter(RFgroup==0) %>% select(s) %>% pull()
  dfB = df %>% filter(RFgroup==2) %>% select(s) %>% pull()
  #realResult = wilcox.test(dfA, dfB)$p.value
  realResult = mean(dfA) - mean(dfB)

  # Permutations
  set.seed(1)
  permutedResults = 1:numPermutations
  for(i in 1:numPermutations){
    dfA = df %>% mutate(RFgroup = sample(RFgroup)) %>% filter(RFgroup==0) %>% select(s) %>% pull()
    dfB = df %>% mutate(RFgroup = sample(RFgroup)) %>% filter(RFgroup==2) %>% select(s) %>% pull()
    #permutedResults[i] = wilcox.test(dfA, dfB)$p.value
    permutedResults[i] = mean(dfA) - mean(dfB)
  }

  Zscore = abs(realResult - mean(permutedResults)) / sd(permutedResults)

  if (realResult < 0){
    pvalue = sum(permutedResults < realResult) / numPermutations
  }
  else{
    pvalue = sum(permutedResults > realResult) / numPermutations
  }

  return(list(realResult, permutedResults, mean(permutedResults), median(permutedResults), sd(permutedResults), Zscore, pvalue))
}

uncorrectedP = rep(NA, 7*3)
iterator = 1

for(visit in 1:7){
  for(cluster in 1:3){
    uncorrectedP[iterator] = ASVgroupPermutationTest("upper jaw, lingual", featureClustering, visit, cluster, 999)[[7]]
    iterator = iterator + 1
  }
}

correctedP = p.adjust(uncorrectedP, "BH")

matrix(uncorrectedP, nrow=3, ncol=7)
matrix(correctedP, nrow=3, ncol=7)
```